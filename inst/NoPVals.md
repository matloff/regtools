
#  Clearing the Confusion: the Case Against Significance Tests

In 2016, the august American Statistical Association (ASA) issued  the
first technical policy statement in its 177-year history.  It called
into question one of core concepts in statistical methodology,
*significance testing* (ST).

To be sure, the position paper did not quite ban ST.  It was, in many
ways, the proverbial "camel created by a committee"; as some on the
committee still believe in ST, the paper stopped short of a ban.  Yet,
the paper offered no use case in which ST *should* be used; their
silence on that may be considered "significant" (pun intended).

As the paper points out, these concerns about ST were not new.  They
were, for instance, explained in 
[a popular elementary stat text](https://wwnorton.com/books/9780393929720) and 
even 
[entire books](https://www.press.umich.edu/186351/cult_of_statistical_significance).  (See also 
[my paper](https://academic.oup.com/ee/article/20/5/1246/2480617).)

Indeed, most statisticians had aware of the flaws of ST, but had
continued to use them and teach them, whether out of habit, an
appreciation of the elegance of the theory, or just plain convenience.

Then what changed?  Among other things, 
[a paper by John Ionnidis[(https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124), 
provcatively titled, "Why Most Published Research Findings Are 
False," shook things up . Though it was mainly on the related issue of 
*p-hacking* rather on the core of ST, it caused the statistical
community to take a long-overdue look at statistical practice.

## Review of the Mechanics of ST


(under construction)


